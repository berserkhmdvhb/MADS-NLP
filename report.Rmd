---
title: "NLP Project Report"
author: "Najada Feimi, Elnaz Khaveh, Hamed Vaheb"
date: "12/29/2022"
output:
  html_document:
    number_sections: yes
    toc: yes
    code_folding: hide
    theme: readable
    highlight: haddock
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# **Import Libraries**

```{r message = FALSE, warning = FALSE}
library(dplyr)
library(ggplot2)
library(broom)
library(janitor)
library(renv)
library(purrr)
library(tm)
library(SnowballC)
library(RColorBrewer)
library(ggplot2)
library(wordcloud)
library(biclust)
library(cluster)
library(igraph)
library(fpc)
library(magrittr)
library(rmarkdown)
library(textreuse)
library(slam)



library(plotly)
library(htmltools)
library(klaR)
library(tidyr)
library(stringr)
```

# **Introduction**

One of the most important tasks of Natural Language Processing is text similarity. Text Similarity is the process of comparing a piece of text with another and finding the similarity between them. It's basically about determining the degree of closeness of the text. 

For this purpose, we chose a dataset of speeches from american presidents. Using Natural Language processing tools we firstly converted the data to DataFrame. Then we started to preprocess the data, firstly uniforming the documents using by removing punctuation, numbers and transforming it to lowercase. Then we continue with  basic NLP text modifications like removal of stop words, tokenizing, lemmatization.

On the next step we used "tm" library tools in order to find the term similarity and later on we separated each speech into separate documents and used "textresuse" library to measure the document similarity and additionally, we visualized the results each time.

## **Describe Dataset**
The dataset used for this project is president speeches obtained from [this link](https://www.presidency.ucsb.edu/documents/presidential-documents-archive-guidebook/inaugural-addresses).

Using the following script in Python, we first created a dataframe of the website's speeches:

```
import requests
from bs4 import BeautifulSoup
import pandas as pd

# Scrapes transcripts for inaugural addresses


def get_urls(url):
    '''Returns list of transcript urls'''
    
    page = requests.get(url).text
    soup=BeautifulSoup(page, 'lxml')
    url_table = soup.find("table", class_='table').find_all("a")
    return [u["href"] for u in url_table]

urls = get_urls("https://www.presidency.ucsb.edu/documents/presidential-documents-archive-guidebook/inaugural-addresses")

transcripts = pd.DataFrame()

def get_transcripts(urls, transcripts):
    for u in urls:
        page = requests.get(u).text
        soup = BeautifulSoup(page, 'lxml')
        t_president = soup.find("h3", class_="diet-title").text
        t_year = soup.find("span", class_="date-display-single").text.split(',')[1].strip()
        t_content = soup.find("div", class_="field-docs-content").text
        record = {
            'president' : t_president,
            'year' : t_year,
            'content' : t_content
        }
        transcripts = transcripts.append(record, ignore_index=True)
    return transcripts

data = get_transcripts(urls,transcripts)
data.to_csv("us_presidents_transcripts.csv", sep="|")
```

In what follows, we load the dataframe:

```{r}
df <- read.csv("https://raw.githubusercontent.com/berserkhmdvhb/MADS-NLP/main/data/presidents-speech.csv")
```

```{r}
df |> dplyr::glimpse()
```
In our datframe, we have 4 columns, X which is the index, president which displays the name of the presidents. Year which shows the year in which the president gave the speech and last one is the content. In the content field we have the content for each speech.

Below we check some details about the dataframe we created.
It has 59 records. The earliest speech it has dates in 1789 and the latest in 2021.

```{r}
df |> summary()
```

In what follows, text files are generated from each row of dataframe and are stored in "texts" folder:

```{r}
#presidents <- df[["president"]]|> unique() |>as.list()

for(i in 1:nrow(df)) {       # for-loop over rows
  df_i <- df[i, ]
  name <- df_i$president
  year <- df_i$year
  text <- df_i$content |> stringr::str_trim()
  file_name <- paste(as.character(year), 
                     as.character(name), 
                     sep="-")
  file_name <- paste(file_name, ".txt", 
                     sep="")
  loc <- paste("./data/texts/", file_name, sep="")
  writeLines(text, loc)
}  
```


```{r}
loc <- "./data/texts/"
docs <- tm::VCorpus(DirSource(loc)) 
summary(docs) 
```

```{r}
inspect(docs[1])
```
Here we check the content of document one which should refer to the speech by Goerge Washington in 1789. We will use the content from this document as a demonstation for the preprocessing part.
```{r}
writeLines(as.character(docs[1]))
```

## **Goal and Procedure**

This project is dedicated to investigating text similarity between speeches from different presidents of US during various years, starting from 1789 and ending with 2021.

In [Preprocessing](#preprocess) section, numerous text mining tasks are implemented on all the documents.

In [Term Similarity](#termsim) section, frequency of different terms in documents are analyzed and visualized.

In [Doc Similarity](#docsim), similarity between documents is measured, analyzed, and  visualized.

In [Conclusion](#conclusion), main findings are summarized.

The github repository for this package can be found in this [link](https://github.com/berserkhmdvhb/MADS-NLP)

# **Preprocessing** {#preprocess}

The [tm](https://www.rdocumentation.org/packages/tm) is a framework for text mining applications within R.
Most functions used henceforth stems from this package.

## **Remove punctuation**

The punctuation removal process will help to treat each text equally. For example, the word data and data! are treated equally after the process of removal of punctuations.
After the removal we print the content of the first document one more time and check the results.
The sentences are devided by , and are within quotes ,but inside the quotes the punctuation is removed.
```{r}
docs <- tm::tm_map(docs,removePunctuation)   
writeLines(as.character(docs[1])) 
```

## **Remove special characters**
Secondly, we remove all special characters. For this purpose we use gsub which replaces the special characters dictated by us with space. We check the document one more time.

```{r}
for (j in seq(docs)) {
    docs[[j]] <- gsub("/", " ", docs[[j]])
    docs[[j]] <- gsub("@", " ", docs[[j]])
    docs[[j]] <- gsub("\\|", " ", docs[[j]])
    docs[[j]] <- gsub("\u2028", " ", docs[[j]])  # This is an ascii character that did not translate, so it had to be removed.
}
writeLines(as.character(docs[1]))
```

## **Remove numbers**

In this step, in order to make the text more uniform we remove all the numerical forms. For doing so, there exists a function from the tm library.
```{r}
docs <- tm::tm_map(docs, removeNumbers)   
writeLines(as.character(docs[1])) 
```

## **Convert to lowercase**

Again, serving the uniformity purposed we tranform all the uppercase to lowercase. Words like Book and book mean the same but when not converted to the lower case those two are represented as two different words in the vector space model (resulting in more dimensions).

Checking the first document below we see that now the first word of the speech, respectively "Felowcitizens" starts with a lowercase.
```{r}
docs <- tm::tm_map(docs, tolower)
docs <- tm::tm_map(docs, PlainTextDocument)
DocsCopy <- docs
writeLines(as.character(docs[1])) 
```

## **Remove "stopwords"** 

Stop words are available in abundance in any human language. By removing these words, we remove the low-level information from our text in order to give more focus to the important information.

```{r}
# For a list of the stopwords, see:   
length(stopwords("english"))   
docs <- tm::tm_map(docs, removeWords, stopwords("english"))   
docs <- tm::tm_map(docs, PlainTextDocument)
writeLines(as.character(docs[1]))
```
  
## **Remove particular stopwords** 

```{r}
#docs <- tm::tm_map(docs, removeWords, c("syllogism", "tautology"))   
# Just remove the words "syllogism" and "tautology". 
# These words don't actually exist in these texts. But this is how you would remove them if they had.
```

## **Retain compouned words** 


If you wish to preserve a concept which is only apparent as a collection of two or more words, then you can combine them or reduce them to a meaningful acronym before you begin the analysis. Here, we are using examples that are particular to qualitative data analysis.

```{r}
for (j in seq(docs))
{
  docs[[j]] <- gsub("fake news", "fake_news", docs[[j]])
  docs[[j]] <- gsub("inner city", "inner-city", docs[[j]])
  docs[[j]] <- gsub("politically correct", "politically_correct", docs[[j]])
}
docs <- tm_map(docs, PlainTextDocument)
```

## **Strip unnecessary white space**



```{r}
docs <- tm_map(docs, stripWhitespace)
writeLines(as.character(docs[1]))

docs <- tm_map(docs, PlainTextDocument)
```

## **Stemming**
The stemDocument from the tm package performs stemming on the documents. However, it doesn't perform all the needed stemming transformations correctly. To resolve this, first a copy of corpus is stored and then apply the stemDocument

```{r}
dictCorpus <- docs
docs <- tm_map(docs, stemDocument)
writeLines(as.character(docs[1]))
```

Secondly, stemCompletion can be used from tm to account for the wrong trasnformations and complete the stems by referencing to the copied corpus (also called dictionary). However, the original stemCompletion function replaces empty strings with unsolicited never existed words. To avoid this, we defined a modified version of stemCompletion.

```{r}
stemCompletion_mod <- function(x, dictionary) {
   x <- unlist(strsplit(as.character(x), " "))
   x <- x[x != ""]
   x <- stemCompletion(x, dictionary=dictionary)
   x <- paste(x, sep="", collapse=" ")
   PlainTextDocument(stripWhitespace(x))
 }

```



```{r}
docs <- lapply(docs, stemCompletion_mod, dictionary=dictCorpus)
docs <- Corpus(VectorSo1urce(docs))
writeLines(as.character(docs[1]))
```



## **Type Check**

Be sure to use the following script once you have completed preprocessing.
This tells R to treat the preprocessed documents as text documents.


```{r}
docs <- tm::tm_map(docs, stripWhitespace)
writeLines(as.character(docs[1]))
```
```{r}
nrow(df)
```
In the below piece of code, we save the preprocessed documents into another folder because later on we need to reuse the results to measure the document similarity using another library which is textreuse.

```{r}
#Elnaz

for(i in 1:nrow(df)) {       # for-loop over rows
  df_i <- df[i, ]
  name <- df_i$president
  year <- df_i$year
  text <- df_i$content
  file_name <- paste(as.character(year), 
                     as.character(name), 
                     sep="-")
  file_name <- paste(file_name, ".txt", 
                     sep="")
  loc <- paste("./data/pre_processed/", file_name, sep="")
  writeLines(as.character(docs[[i]]), loc)
}
```



## **Create Doc Term Matrix**

A document-term matrix or term-document matrix is a mathematical matrix that describes the frequency of terms that occur in a collection of documents. In a document-term matrix, rows correspond to documents in the collection and columns correspond to terms.

```{r}
dtm <- tm::DocumentTermMatrix(docs)   
dtm 
```

Storing transpose of matrix

```{r}
tdm <- tm::TermDocumentMatrix(docs)   
tdm  
```


## **Organize by frequency**


```{r}
freq <- colSums(as.matrix(dtm))   
length(freq)   
```

```{r}
ord <- order(freq)
m <- as.matrix(dtm)   
dim(m)  
```


Store the matrix to memory
```{r}
#write.csv(m, file="DocumentTermMatrix.csv")   
```


## **Remove sparse words** {#removesparse}

We remove sparse words putting a 20% sparsity thresshold, and when we check our results, the sparsity for our matrix is 12%. 
```{r}
#  Start by removing sparse terms:   
dtms <- removeSparseTerms(dtm, 0.2) # This makes a matrix that is 20% empty space, maximum.   
dtms
```

# **Term Similarity** {#termsim}

We save the matrix as the frequency of the terms.

```{r}
freq <- colSums(as.matrix(dtm))
```

Least frequent

We print the head of the frequency table. Our table is increasing. So the ones appearing at the head have 1 frequency therefore the smallest possible number and it increases until at the tail we have the most frequent words.

```{r}
head(table(freq), 20) 
```

The top number is the frequency with which words appear and the bottom number reflects how many words appear that frequently.

Most frequent:

```{r}
tail(table(freq), 40) 
```

Below we show a table of the terms we selected when we removed sparse terms in subsection [Remove sparse words](#removesparse)
We print the 20 first most frequent terms. 

```{r}
freq <- sort(colSums(as.matrix(dtm)), decreasing=TRUE)     
freq |> head(20)
```

Below we identify all terms that appear frequently.
```{r}
findFreqTerms(dtm, lowfreq=50) |> head(20)
```
Another approach to perform the same task:

```{r}
wf <- data.frame(word=names(freq), freq=freq)   
head(wf) 
```


## **Word Frequency Plot**

Now it is time to visualize our results to better understand and perceive them. Using ggplot we show a bar plot with words that appear more than 200 times. In the x-axis we can see clearly which are these words. They are presented in the root form since we applied stemming.

```{r}
p <- ggplot(subset(wf, freq>200), aes(x = reorder(word, -freq), y = freq)) + 
  geom_bar(stat = "identity") + 
  theme(axis.text.x=element_text(angle=45, hjust=1))

p   
```

## **Relationships Between Terms**

Here we find the correlations between the terms as if 2 words are always appeared together in a text then the correlation between them would be 1. The correlation limit is considered as 0.75:

```{r}
tm::findAssocs(dtm, c("government" , "states"), corlimit=0.75)
```


```{r}
findAssocs(dtms, "government", corlimit=0.70) # specifying a correlation limit of 0.95   
```

## **Word Clouds**

Plot words that occur at least 25 times.

Colorized version:

In this part the word clouds are visualized. The bigger the size of the word in the word cloud, the more frequent it is. Also words are clustered based one frequency with different colors.
```{r}
set.seed(142)   
wordcloud::wordcloud(names(freq), freq, min.freq=20, scale=c(5, .1), colors=brewer.pal(6, "Dark2")) 
```

Plot words that occur at least 100 times.

We use the same way of plotting, therefore the size and color stand for the same reasons.
```{r}
set.seed(142)   
dark2 <- brewer.pal(6, "Dark2")   
wordcloud::wordcloud(names(freq), freq, max.words=100, rot.per=0.2, colors=dark2)  
```

## **Clustering by Term Similarity**

### **Hierarchal Clustering**

To do the Hierarchal clustering, first we should find the distance between words and for this purpose we used Euclidian norm and then clustered based on those distances.

```{r}
d <- dist(t(dtms), method="euclidian")   
fit <- hclust(d=d, method="complete")   # for a different look try substituting: method="ward.D"
fit   
```
Dendrograms are the plots used to visualize the hierarchal clustering. If the height of the line joining 2 terms is smaller, it shows that they are more similar, whereas Higher lines in dendrograms indicate larger distance between the clusters.

```{r}
plot(fit, hang=-1)
```
And here the red boxes show the 6 clusters:

```{r}
plot.new()
plot(fit, hang=-1)
groups <- cutree(fit, k=6)   # "k=" defines the number of clusters you are using   
rect.hclust(fit, k=6, border="red") # draw dendogram with red borders around the 6 clusters
```


### **K-means Clustering**

To do the k-means clustering, first we should find the distance between words and for this purpose we used 3 different norms("Euclidian", "Manhattan","Maximum") and the clustered based on them.

In what follows, there are clusplots for K-Means clustering with different norms. In clusplots, each ellipse indicate a Principal Component. At the bottom of each plot we can see the percentage of the point variability explained by these components.

**Norm: Euclidean**
```{r}
d <- dist(t(dtms), method="euclidean")   
kfit <- kmeans(d, 2)   
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)
```

```{r}
d <- dist(t(dtms), method="euclidian")   
kfit <- kmeans(d, 4)   
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)
```

**Norm: Manhattan**

```{r}
d <- dist(t(dtms), method="manhattan")   
kfit <- kmeans(d, 4)   
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)
```

```{r}
d <- dist(t(dtms), method="manhattan")   
kfit <- kmeans(d, 2)   
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)
```

**Norm: Maximum**


```{r}
d <- dist(t(dtms), method="maximum")   
kfit <- kmeans(d, 4)   
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)
```

```{r}
d <- dist(t(dtms), method="maximum")   
kfit <- kmeans(d, 2)   
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)
```

TODO: Use other norms to perform K-means...done(Elnaz)


# **Doc Similarity** {#docsim}

For this section, [textreuse](https://docs.ropensci.org/textreuse/) library, is used.

We compare documents in a pairwise manner and use jaccard similarity to measure the similarity between them. After doing so, a score is calculated for each pair as shown in the results:
```{r}
#loc <- "./data/texts"
#docs <- tm::VCorpus(DirSource(loc)) 

loc <- "./data/pre_processed/"
corpus <- TextReuseCorpus(dir=loc)


comparisons <- pairwise_compare(corpus, jaccard_similarity)
compare_df <- pairwise_candidates(comparisons)
compare_df <- as.data.frame(compare_df, 
                            col.names = names(compare_df))
#compare_df <- compare_df[order(compare_df$score,decreasing=TRUE)]
compare_df <- compare_df[order(compare_df$score,decreasing=TRUE),]
compare_df |> head(3)
```

```{r}
#Najada
corpus
writeLines(as.character(corpus[1]))
```


## **Similarity Score Plot**

**Similarity Measure: Jaccard Similarity**

Now our goal is to visualize the similarities. For this purpose, we build a 3D plot which in x-axis has one speech and y-axis has another and on z-axis the scores. In order to visualize in a clean way, we used only the first 30 ones which are the most similar ones and we used only the initials on the presidents. Taken into account that these speeches were made from some of the most important and well-known American Presidents, the plot does not lose its explanatory purposes. 

Each score is presented with a ball and the colors represent the clusters. The pairs with similar scores are painted the same color.

```{r}
#Choosing only the first 50 rows because otherwise the plot becomes unreadable since there are too many points
compare_df_viz <- compare_df[1:30, ]
```


```{r}
# Converting names to initials
compare_df_viz$a <- gsub("(?<=[A-Z])[^A-Z]+", "", compare_df_viz$a ,perl = TRUE)

compare_df_viz$b <- gsub("(?<=[A-Z])[^A-Z]+", "", compare_df_viz$b ,perl = TRUE)
```


```{r}
fig <- plot_ly(compare_df_viz, x = ~a, y = ~b, z = ~score, color=~score, size=~score)
fig <- fig |> add_markers()
fig <- fig |> layout(scene = list(xaxis = list(title = 'Doc1'),
                     yaxis = list(title = 'Doc2'),
                     zaxis = list(title = 'Similarity Score')
                     ))

fig

```



**Similarity Measure: Ratio of Matches**

Here we use another similarity measure. The first one was based on Jaccard Similarity and this one is based in Ratio of matches.
The method is the same, but the results are slightly different. Here we have higher similarity measures. 

For this reason, this time we plot 50 most similar cases and with smaller ball size.
```{r}
loc <- "./data/pre_processed/"
corpus <- TextReuseCorpus(dir=loc)


comparisons_rom <- pairwise_compare(corpus, ratio_of_matches)
compare_df_rom <- pairwise_candidates(comparisons_rom)
compare_df_rom <- as.data.frame(compare_df_rom, 
                            col.names = names(compare_df_rom))
#compare_df <- compare_df[order(compare_df$score,decreasing=TRUE)]
compare_df_rom <- compare_df_rom[order(compare_df_rom$score,decreasing=TRUE),]
compare_df_rom |> head(3)
```
```{r}
compare_df_rom_viz <- compare_df_rom[1:50, ]
```


```{r}
compare_df_rom_viz$a <- gsub("(?<=[A-Z])[^A-Z]+", "", compare_df_rom_viz$a ,perl = TRUE)

compare_df_rom_viz$b <- gsub("(?<=[A-Z])[^A-Z]+", "", compare_df_rom_viz$b ,perl = TRUE)
```

```{r}
fig <- plot_ly(compare_df_rom_viz, x = ~a, y = ~b, z = ~score, color=~score, size=~score)
fig <- fig |> add_markers()
fig <- fig |> layout(scene = list(xaxis = list(title = 'Doc1'),
                     yaxis = list(title = 'Doc2'),
                     zaxis = list(title = 'Similarity Score')
                     ))

fig



```




## **Extra Material: Distance Matrix**

Since the [textreuse](https://docs.ropensci.org/textreuse/) library doesn't output a distance matrix, and instead we can only have a dataframe with three columns, two of which contain documents' names, and the third one contain their similarity score (computed from Jaccard similarity), we implemented the transformation of the mentioned dataframe to a distance matrix. To achieve this, we pivot the score dataframe in the following manner:

```{r} 
distance_df <- compare_df |> pivot_wider(names_from=a, values_from=score)

distance_df <- replace(distance_df, is.na(distance_df), 0)

distance_mat <- data.matrix(distance_df)
```



Moreover, the library doesn't provide a function to compute cosine similarity between any pair of documents of the corpus, in below we implemented computation of cosine similarity between two given documents of the corpus, and then construct a distance matrix for all documents of the corpus.


```{r}
# compute cosine similarity between two documents
dtms[,1]
cosine_sim <- tcrossprod_simple_triplet_matrix(dtms[,1], dtms[,2])/sqrt(row_sums(dtms[,2]^2) %*% t(row_sums(dtms[,1]^2)))

```

```{r}
# construct cosine distance matrix
cosine_dist_mat <- 1 - crossprod_simple_triplet_matrix(dtms)/(sqrt(col_sums(dtms^2) %*% t(col_sums(dtms^2))))

cosine_dist_mat
```


# **Conclusion** {#conclusion}

Firstly, it is worth mentioning our understanding behind the most used terms. 
As shown above, the first most frequent word was "WILL". We believe, from our general knowledge as well, that this is a significant word in politician speeches. Politicians make promises, and when one promises he/she usually uses the future tense. We also notice that words like: Govern which is the stem for government, state and nation are frequently used. Again these are typical words from a state leader and we also expected them to be used frequently.

Secondly, regarding the document similarity we used two different measurement methods: respectively Jaccard Similarity and Ratio of Matches. They both presented low scores as the result, but at least for the first ranked pairs the results from Ratio of Matches are twice as high in comparison with the first method. //