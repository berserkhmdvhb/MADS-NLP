---
title: "NLP Project Report"
author: "Najada Feimi, Elnaz Khaveh, Hamed Vaheb"
date: "12/29/2022"
output:
  html_document:
    number_sections: yes
    toc: yes
    code_folding: hide
    theme: readable
    highlight: haddock
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

# **Import Libraries**

```{r message = FALSE, warning = FALSE}
library(knitr)
library(dplyr)
library(ggplot2)
library(broom)
library(janitor)
library(renv)
library(purrr)
library(tm)
library(SnowballC)
library(RColorBrewer)
library(ggplot2)
library(wordcloud)
library(biclust)
library(cluster)
library(igraph)
library(fpc)
library(magrittr)
#packages <- c("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust", "cluster", "igraph", "fpc", "knitr", "dplyr", "broom", "janitor", "renv", "purrr")
#install.packages(packages, dependencies = TRUE)

```

# **Introduction**

## **Load Dataset**

```{r}
df <- read.csv("https://raw.githubusercontent.com/berserkhmdvhb/MADS-NLP/main/data/presidents-speech.csv")
```

```{r}
df |> dplyr::glimpse()
```

```{r}
df |> summary()
```

In what follows, text files are generated from each row of dataframe and are stored in "texts" folder:

```{r}
#presidents <- df[["president"]]|> unique() |>as.list()



for(i in 1:nrow(df)) {       # for-loop over rows
  df_i <- df[i, ]
  name <- df_i$president
  year <- df_i$year
  text <- df_i$content
  file_name <- paste(as.character(year), 
                     as.character(name), 
                     sep="-")
  file_name <- paste(file_name, ".txt", 
                     sep="")
  loc <- paste("/home/hamed/Documents/R/MADS-NLP/data/texts/", file_name, sep="")
  #writeLines(text, loc)
}  
  
```

```{r}
loc <- "/home/hamed/Documents/R/MADS-NLP/data/texts"
docs <- VCorpus(DirSource(loc)) 
summary(docs) 
```

```{r}
inspect(docs[1])
```

```{r}
writeLines(as.character(docs[1]))
```

## **Goal and Procedure**

This project is dedicated to investigating text similarity between speeches from different presidents of US during various years, starting from 1789 and ending with 2021.

In [Preprocessing](#preprocess) section, numerous text mining tasks are implemented on all the docs.

In [Word Frequency](#wordfreq) section, frequency of different terms in documents are analyzed and visualized.


# **Preprocessing** {#preprocess}

## **Remove punctuation**

```{r}
docs <- tm_map(docs,removePunctuation)   
writeLines(as.character(docs[1])) 
```


## **Remove special characters**

```{r}
for (j in seq(docs)) {
    docs[[j]] <- gsub("/", " ", docs[[j]])
    docs[[j]] <- gsub("@", " ", docs[[j]])
    docs[[j]] <- gsub("\\|", " ", docs[[j]])
    docs[[j]] <- gsub("\u2028", " ", docs[[j]])  # This is an ascii character that did not translate, so it had to be removed.
}
writeLines(as.character(docs[1]))
```

## **Remove numbers**

```{r}
docs <- tm_map(docs, removeNumbers)   
writeLines(as.character(docs[1])) 
```

## **Converting to lowercase**

```{r}
docs <- tm_map(docs, tolower)   
docs <- tm_map(docs, PlainTextDocument)
DocsCopy <- docs
writeLines(as.character(docs[1])) 
```

## **Remove "stopwords"** 

```{r}
# For a list of the stopwords, see:   
length(stopwords("english"))   
docs <- tm_map(docs, removeWords, stopwords("english"))   
docs <- tm_map(docs, PlainTextDocument)
writeLines(as.character(docs[1]))
```
  
## **Remove particular stopwords** 

```{r}
#docs <- tm_map(docs, removeWords, c("syllogism", "tautology"))   
# Just remove the words "syllogism" and "tautology". 
# These words don't actually exist in these texts. But this is how you would remove them if they had.
```

## **Retain compouned words** 


If you wish to preserve a concept is only apparent as a collection of two or more words, then you can combine them or reduce them to a meaningful acronym before you begin the analysis. Here, I am using examples that are particular to qualitative data analysis.

```{r}
for (j in seq(docs))
{
  docs[[j]] <- gsub("fake news", "fake_news", docs[[j]])
  docs[[j]] <- gsub("inner city", "inner-city", docs[[j]])
  docs[[j]] <- gsub("politically correct", "politically_correct", docs[[j]])
}
docs <- tm_map(docs, PlainTextDocument)
```

**Remove common word endings**
Common words ending e.g. “ing”, “es”, “s”

```{r}
## Note: I did not run this section of code for this particular example.
docs_st <- tm_map(docs, stemDocument)   
docs_st <- tm_map(docs_st, PlainTextDocument)
writeLines(as.character(docs_st[1])) # Check to see if it worked.
# docs <- docs_st
```

## **Strip unnecesary whitespace**

```{r}
docs <- tm_map(docs, stripWhitespace)
writeLines(as.character(docs[1]))
```

## **Type Check**

Be sure to use the following script once you have completed preprocessing.
This tells R to treat the preprocessed documents as text documents.


```{r}
docs <- tm_map(docs, stripWhitespace)
writeLines(as.character(docs[1]))
```

## **Create Doc Term Matrix**



```{r}
dtm <- DocumentTermMatrix(docs)   
dtm 
```

Storing transpose of matrix

```{r}
tdm <- TermDocumentMatrix(docs)   
tdm  
```


## **Organize by frequency**


```{r}
freq <- colSums(as.matrix(dtm))   
length(freq)   
```

```{r}
ord <- order(freq)
m <- as.matrix(dtm)   
dim(m)  
```


Store the matrix to memory
```{r}
#write.csv(m, file="DocumentTermMatrix.csv")   
```


## **Remove sparse words** {#removesparse}

```{r}
#  Start by removing sparse terms:   
dtms <- removeSparseTerms(dtm, 0.2) # This makes a matrix that is 20% empty space, maximum.   
dtms
```

# **Word Frequency** {#wordfreq}

```{r}
freq <- colSums(as.matrix(dtm))
```

Least frequent

```{r}
head(table(freq), 20) 
```

 The top number is the frequency with which words appear and the bottom number reflects how many words appear that frequently.

Most frequent:

```{r}
tail(table(freq), 40) 
```



View a table of the terms we selected when we removed sparse terms in subsection [Remove sparse words](#removesparse)

```{r}
freq <- sort(colSums(as.matrix(dtm)), decreasing=TRUE)     
freq |> head(20)
```


Identify all terms that appear frequently
```{r}
findFreqTerms(dtm, lowfreq=50) |> head(20)
```
Another approach to perform the same task:

```{r}
wf <- data.frame(word=names(freq), freq=freq)   
head(wf) 
```


## **Plot**


```{r}
p <- ggplot(subset(wf, freq>100), aes(x = reorder(word, -freq), y = freq)) + 
  geom_bar(stat = "identity") + 
  theme(axis.text.x=element_text(angle=45, hjust=1))

p   
```

# **Relationships Between Terms**

```{r}
findAssocs(dtm, c("government" , "states"), corlimit=0.75) # specifying a correlation limit of 0.85 
```


```{r}
findAssocs(dtms, "government", corlimit=0.70) # specifying a correlation limit of 0.95   
```

**Word Clouds**

Plot words that occur at least 25 times.

Colorized version:

```{r}
set.seed(142)   
wordcloud(names(freq), freq, min.freq=20, scale=c(5, .1), colors=brewer.pal(6, "Dark2")) 
```

Plot words that occur at least 100 times.

```{r}
set.seed(142)   
dark2 <- brewer.pal(6, "Dark2")   
wordcloud(names(freq), freq, max.words=100, rot.per=0.2, colors=dark2)  
```

# **Clustering by Term Similarity**

## **Hierarchal Clustering**


```{r}
d <- dist(t(dtms), method="euclidian")   
fit <- hclust(d=d, method="complete")   # for a different look try substituting: method="ward.D"
fit   
```

```{r}
plot(fit, hang=-1)
```

```{r}
plot.new()
plot(fit, hang=-1)
groups <- cutree(fit, k=6)   # "k=" defines the number of clusters you are using   
rect.hclust(fit, k=6, border="red") # draw dendogram with red borders around the 6 clusters
```


## **K-means Clustering**

```{r}
d <- dist(t(dtms), method="euclidian")   
kfit <- kmeans(d, 2)   
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)
```
```{r}
d <- dist(t(dtms), method="euclidian")   
kfit <- kmeans(d, 2)   
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)
```