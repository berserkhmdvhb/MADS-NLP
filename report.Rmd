---
title: "NLP Project Report"
author: "Najada Feimi, Elnaz Khaveh, Hamed Vaheb"
date: "12/29/2022"
output:
  html_document:
    number_sections: yes
    toc: yes
    code_folding: hide
    theme: readable
    highlight: haddock
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

# **Import Libraries**

```{r message = FALSE, warning = FALSE}
library(knitr)
library(dplyr)
library(ggplot2)
library(broom)
library(janitor)
library(renv)
library(purrr)
library(tm)
library(SnowballC)
library(RColorBrewer)
library(ggplot2)
library(wordcloud)
library(biclust)
library(cluster)
library(igraph)
library(fpc)
library(magrittr)
#packages <- c("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust", "cluster", "igraph", "fpc", "knitr", "dplyr", "broom", "janitor", "renv", "purrr")
#install.packages(packages, dependencies = TRUE)

```

# **Introduction**

## **Load Dataset**

```{r}
df <- read.csv("https://raw.githubusercontent.com/berserkhmdvhb/MADS-NLP/main/data/presidents-speech.csv")
```

```{r}
df |> dplyr::glimpse()
```

```{r}
df |> summary()
```

```{r}
#presidents <- df[["president"]]|> unique() |>as.list()



for(i in 1:nrow(df)) {       # for-loop over rows
  df_i <- df[i, ]
  name <- df_i$president
  year <- df_i$year
  text <- df_i$content
  file_name <- paste(as.character(year), 
                     as.character(name), 
                     sep="-")
  file_name <- paste(file_name, ".txt", 
                     sep="")
  loc <- paste("/home/hamed/Documents/R/MADS-NLP/data/texts/", file_name, sep="")
  #writeLines(text, loc)
}  
  
```

```{r}
loc <- "/home/hamed/Documents/R/MADS-NLP/data/texts"
docs <- VCorpus(DirSource(loc)) 
summary(docs) 
```

```{r}
inspect(docs[1])
```

```{r}
writeLines(as.character(docs[1]))
```

## **Goal and Procedure**

TO BE FILLED


# **Preprocessing**

## **Remove punctuation**

```{r}
docs <- tm_map(docs,removePunctuation)   
writeLines(as.character(docs[1])) 
```


## **Remove special characters**

```{r}
for (j in seq(docs)) {
    docs[[j]] <- gsub("/", " ", docs[[j]])
    docs[[j]] <- gsub("@", " ", docs[[j]])
    docs[[j]] <- gsub("\\|", " ", docs[[j]])
    docs[[j]] <- gsub("\u2028", " ", docs[[j]])  # This is an ascii character that did not translate, so it had to be removed.
}
writeLines(as.character(docs[1]))
```

## **Remove numbers**

```{r}
docs <- tm_map(docs, removeNumbers)   
writeLines(as.character(docs[1])) 
```

## **Converting to lowercase**

```{r}
docs <- tm_map(docs, tolower)   
docs <- tm_map(docs, PlainTextDocument)
DocsCopy <- docs
writeLines(as.character(docs[1])) 
```

## **Removing "stopwords"** 

```{r}
# For a list of the stopwords, see:   
length(stopwords("english"))   
docs <- tm_map(docs, removeWords, stopwords("english"))   
docs <- tm_map(docs, PlainTextDocument)
writeLines(as.character(docs[1]))
```
  
## **Removing particular stopwords** 

```{r}
#docs <- tm_map(docs, removeWords, c("syllogism", "tautology"))   
# Just remove the words "syllogism" and "tautology". 
# These words don't actually exist in these texts. But this is how you would remove them if they had.
```

## **Combining words that should stay together** 


If you wish to preserve a concept is only apparent as a collection of two or more words, then you can combine them or reduce them to a meaningful acronym before you begin the analysis. Here, I am using examples that are particular to qualitative data analysis.

```{r}
for (j in seq(docs))
{
  docs[[j]] <- gsub("fake news", "fake_news", docs[[j]])
  docs[[j]] <- gsub("inner city", "inner-city", docs[[j]])
  docs[[j]] <- gsub("politically correct", "politically_correct", docs[[j]])
}
docs <- tm_map(docs, PlainTextDocument)
```

**Removing common word endings (e.g., “ing”, “es”, “s”)**

```{r}
## Note: I did not run this section of code for this particular example.
docs_st <- tm_map(docs, stemDocument)   
docs_st <- tm_map(docs_st, PlainTextDocument)
writeLines(as.character(docs_st[1])) # Check to see if it worked.
# docs <- docs_st
```

## **Stripping unnecesary whitespace**

```{r}
docs <- tm_map(docs, stripWhitespace)
writeLines(as.character(docs[1]))
```

## **Type Check**

Be sure to use the following script once you have completed preprocessing.
This tells R to treat the preprocessed documents as text documents.


```{r}
docs <- tm_map(docs, stripWhitespace)
writeLines(as.character(docs[1]))
```

## **Create Doc Term Matrix**



```{r}
dtm <- DocumentTermMatrix(docs)   
dtm 
```

Storing transpose of matrix

```{r}
tdm <- TermDocumentMatrix(docs)   
tdm  
```


## **Organize by frequency**


```{r}
freq <- colSums(as.matrix(dtm))   
length(freq)   
```

```{r}
ord <- order(freq)
m <- as.matrix(dtm)   
dim(m)  
```


Store the matrix to memory
```{r}
#write.csv(m, file="DocumentTermMatrix.csv")   
```


## **Remove infrequent words**

```{r}
#  Start by removing sparse terms:   
dtms <- removeSparseTerms(dtm, 0.2) # This makes a matrix that is 20% empty space, maximum.   
dtms
```

