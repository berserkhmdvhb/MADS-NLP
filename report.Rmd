---
title: "NLP Project Report"
author: "Najada Feimi, Elnaz Khaveh, Hamed Vaheb"
date: "12/29/2022"
output:
  html_document:
    number_sections: yes
    toc: yes
    code_folding: hide
    theme: readable
    highlight: haddock
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# **Import Libraries**

```{r message = FALSE, warning = FALSE}
library(rmarkdown)
library(dplyr)
library(ggplot2)
library(broom)
library(janitor)
library(renv)
library(purrr)
library(tm)
library(SnowballC)
library(RColorBrewer)
library(ggplot2)
library(wordcloud)
library(biclust)
library(cluster)
library(igraph)
library(fpc)
library(magrittr)
library(rmarkdown)
library(textreuse)
library(slam)


library(htmltools)
library(plotly)
library(klaR)
library(tidyr)
#packages <- c("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust", "cluster", "igraph", "fpc", "knitr", "dplyr", "broom", "janitor", "renv", "purrr")
#install.packages(packages, dependencies = TRUE)

```

# **Introduction**

## **Describe Dataset**
The dataset used for this project is president speeches obtained from [this link](https://www.presidency.ucsb.edu/documents/presidential-documents-archive-guidebook/inaugural-addresses).

Using the following script in Python, we first created a dataframe of the website's speeches:


```
import requests
from bs4 import BeautifulSoup
import pandas as pd

# Scrapes transcripts for inaugural addresses


def get_urls(url):
    '''Returns list of transcript urls'''
    
    page = requests.get(url).text
    soup=BeautifulSoup(page, 'lxml')
    url_table = soup.find("table", class_='table').find_all("a")
    return [u["href"] for u in url_table]

urls = get_urls("https://www.presidency.ucsb.edu/documents/presidential-documents-archive-guidebook/inaugural-addresses")

transcripts = pd.DataFrame()

def get_transcripts(urls, transcripts):
    for u in urls:
        page = requests.get(u).text
        soup = BeautifulSoup(page, 'lxml')
        t_president = soup.find("h3", class_="diet-title").text
        t_year = soup.find("span", class_="date-display-single").text.split(',')[1].strip()
        t_content = soup.find("div", class_="field-docs-content").text
        record = {
            'president' : t_president,
            'year' : t_year,
            'content' : t_content
        }
        transcripts = transcripts.append(record, ignore_index=True)
    return transcripts

data = get_transcripts(urls,transcripts)
data.to_csv("us_presidents_transcripts.csv", sep="|")
```

In what follows, we load the dataframe:

```{r}
df <- read.csv("https://raw.githubusercontent.com/berserkhmdvhb/MADS-NLP/main/data/presidents-speech.csv")
```

```{r}
df |> dplyr::glimpse()
```


```{r}
df |> summary()
```

In what follows, text files are generated from each row of dataframe and are stored in "texts" folder:

```{r}
#presidents <- df[["president"]]|> unique() |>as.list()

for(i in 1:nrow(df)) {       # for-loop over rows
  df_i <- df[i, ]
  name <- df_i$president
  year <- df_i$year
  text <- df_i$content
  file_name <- paste(as.character(year), 
                     as.character(name), 
                     sep="-")
  file_name <- paste(file_name, ".txt", 
                     sep="")
  loc <- paste("./data/texts/", file_name, sep="")
  #writeLines(text, loc)
}  
  
```

```{r}
loc <- "./data/texts/"
docs <- tm::VCorpus(DirSource(loc)) 
summary(docs) 
```

```{r}
inspect(docs[1])
```

```{r}
writeLines(as.character(docs[1]))
```

## **Goal and Procedure**

This project is dedicated to investigating text similarity between speeches from different presidents of US during various years, starting from 1789 and ending with 2021.

In [Preprocessing](#preprocess) section, numerous text mining tasks are implemented on all the docs.

In [Term Similarity](#termsim) section, frequency of different terms in documents are analyzed and visualized.

In [Doc Similarity](#docsim), similarity between documents is measured, analyzed, and  visualized.

In [Conclusion](#conclusion), main findings are summarized.

The github repository for this package can be found in this [link](https://github.com/berserkhmdvhb/MADS-NLP)

# **Preprocessing** {#preprocess}

The [tm](https://www.rdocumentation.org/packages/tm) is a framework for text mining applications within R.
Most functions used henceforth stems from this package.

## **Remove punctuation**

```{r}
docs <- tm::tm_map(docs,removePunctuation)   
writeLines(as.character(docs[1])) 
```


## **Remove special characters**

```{r}
for (j in seq(docs)) {
    docs[[j]] <- gsub("/", " ", docs[[j]])
    docs[[j]] <- gsub("@", " ", docs[[j]])
    docs[[j]] <- gsub("\\|", " ", docs[[j]])
    docs[[j]] <- gsub("\u2028", " ", docs[[j]])  # This is an ascii character that did not translate, so it had to be removed.
}
writeLines(as.character(docs[1]))
```

## **Remove numbers**

```{r}
docs <- tm::tm_map(docs, removeNumbers)   
writeLines(as.character(docs[1])) 
```

## **Convert to lowercase**

```{r}
docs <- tm::tm_map(docs, tolower)   
docs <- tm::tm_map(docs, PlainTextDocument)
DocsCopy <- docs
writeLines(as.character(docs[1])) 
```

## **Remove "stopwords"** 

```{r}
# For a list of the stopwords, see:   
length(stopwords("english"))   
docs <- tm::tm_map(docs, removeWords, stopwords("english"))   
docs <- tm::tm_map(docs, PlainTextDocument)
writeLines(as.character(docs[1]))
```
  
## **Remove particular stopwords** 

```{r}
#docs <- tm::tm_map(docs, removeWords, c("syllogism", "tautology"))   
# Just remove the words "syllogism" and "tautology". 
# These words don't actually exist in these texts. But this is how you would remove them if they had.
```

## **Retain compouned words** 


If you wish to preserve a concept is only apparent as a collection of two or more words, then you can combine them or reduce them to a meaningful acronym before you begin the analysis. Here, I am using examples that are particular to qualitative data analysis.

```{r}
for (j in seq(docs))
{
  docs[[j]] <- gsub("fake news", "fake_news", docs[[j]])
  docs[[j]] <- gsub("inner city", "inner-city", docs[[j]])
  docs[[j]] <- gsub("politically correct", "politically_correct", docs[[j]])
}
docs <- tm_map(docs, PlainTextDocument)
```

## **Remove common word endings**
Common words ending e.g. “ing”, “es”, “s”

```{r}
## Note: I did not run this section of code for this particular example.
docs_st <- tm_map(docs, stemDocument)   
docs_st <- tm_map(docs_st, PlainTextDocument)
writeLines(as.character(docs_st[1])) # Check to see if it worked.
# docs <- docs_st
```

## **Strip unnecesary whitespace**

```{r}
docs <- tm_map(docs, stripWhitespace)
writeLines(as.character(docs[1]))
```

## **Type Check**

Be sure to use the following script once you have completed preprocessing.
This tells R to treat the preprocessed documents as text documents.


```{r}
docs <- tm::tm_map(docs, stripWhitespace)
writeLines(as.character(docs[1]))
```
```{r}
nrow(df)
```


```{r}
#Elnaz

for(i in 1:nrow(df)) {       # for-loop over rows
  df_i <- df[i, ]
  name <- df_i$president
  year <- df_i$year
  text <- df_i$content
  file_name <- paste(as.character(year), 
                     as.character(name), 
                     sep="-")
  file_name <- paste(file_name, ".txt", 
                     sep="")
  loc <- paste("./data/pre_processed/", file_name, sep="")
  writeLines(as.character(docs[[i]]), loc)
}
```



## **Create Doc Term Matrix**



```{r}
dtm <- tm::DocumentTermMatrix(docs)   
dtm 
```

Storing transpose of matrix

```{r}
tdm <- tm::TermDocumentMatrix(docs)   
tdm  
```


## **Organize by frequency**


```{r}
freq <- colSums(as.matrix(dtm))   
length(freq)   
```

```{r}
ord <- order(freq)
m <- as.matrix(dtm)   
dim(m)  
```


Store the matrix to memory
```{r}
#write.csv(m, file="DocumentTermMatrix.csv")   
```


## **Remove sparse words** {#removesparse}

```{r}
#  Start by removing sparse terms:   
dtms <- removeSparseTerms(dtm, 0.2) # This makes a matrix that is 20% empty space, maximum.   
dtms
```

# **Term Similarity** {#termsim}

```{r}
freq <- colSums(as.matrix(dtm))
```

Least frequent

```{r}
head(table(freq), 20) 
```

 The top number is the frequency with which words appear and the bottom number reflects how many words appear that frequently.

Most frequent:

```{r}
tail(table(freq), 40) 
```



View a table of the terms we selected when we removed sparse terms in subsection [Remove sparse words](#removesparse)

```{r}
freq <- sort(colSums(as.matrix(dtm)), decreasing=TRUE)     
freq |> head(20)
```


Identify all terms that appear frequently
```{r}
findFreqTerms(dtm, lowfreq=50) |> head(20)
```
Another approach to perform the same task:

```{r}
wf <- data.frame(word=names(freq), freq=freq)   
head(wf) 
```


## **Word Frequency Plot**


```{r}
p <- ggplot(subset(wf, freq>100), aes(x = reorder(word, -freq), y = freq)) + 
  geom_bar(stat = "identity") + 
  theme(axis.text.x=element_text(angle=45, hjust=1))

p   
```

## **Relationships Between Terms**

```{r}
tm::findAssocs(dtm, c("government" , "states"), corlimit=0.75)
```


```{r}
findAssocs(dtms, "government", corlimit=0.70) # specifying a correlation limit of 0.95   
```

## **Word Clouds**

Plot words that occur at least 25 times.

Colorized version:

```{r}
set.seed(142)   
wordcloud::wordcloud(names(freq), freq, min.freq=20, scale=c(5, .1), colors=brewer.pal(6, "Dark2")) 
```

Plot words that occur at least 100 times.

```{r}
set.seed(142)   
dark2 <- brewer.pal(6, "Dark2")   
wordcloud::wordcloud(names(freq), freq, max.words=100, rot.per=0.2, colors=dark2)  
```

## **Clustering by Term Similarity**

### **Hierarchal Clustering**


```{r}
d <- dist(t(dtms), method="euclidian")   
fit <- hclust(d=d, method="complete")   # for a different look try substituting: method="ward.D"
fit   
```

```{r}
plot(fit, hang=-1)
```

```{r}
plot.new()
plot(fit, hang=-1)
groups <- cutree(fit, k=6)   # "k=" defines the number of clusters you are using   
rect.hclust(fit, k=6, border="red") # draw dendogram with red borders around the 6 clusters
```


### **K-means Clustering**

**Norm: Euclidean**
```{r}
d <- dist(t(dtms), method="euclidian")   
kfit <- kmeans(d, 2)   
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)
```

```{r}
d <- dist(t(dtms), method="euclidian")   
kfit <- kmeans(d, 4)   
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)
```




**Norm: ???**

TODO: Use other norms to perform K-means

# **Doc Simlarity** {#docsim}

//TODO: Perform doc similarity using the [textreuse](https://docs.ropensci.org/textreuse/) library, analyze and visualize reults.

```{r}
#loc <- "./data/texts"
#docs <- tm::VCorpus(DirSource(loc)) 

loc <- "./data/pre_processed/"
corpus <- TextReuseCorpus(dir=loc)


comparisons <- pairwise_compare(corpus, jaccard_similarity)
compare_df <- pairwise_candidates(comparisons)
compare_df <- as.data.frame(compare_df, 
                            col.names = names(compare_df))
#compare_df <- compare_df[order(compare_df$score,decreasing=TRUE)]
compare_df <- compare_df[order(compare_df$score,decreasing=TRUE),]
compare_df |> head(3)
```

```{r}
#Najada
corpus
writeLines(as.character(corpus[1]))
```



## **Similarity Score Plot**

**Similarity Measure: Jaccard Similarity**

```{r}
#Choosing only the first 50 rows because otherwise the plot becomes unreadable since there are too many points
compare_df_viz <- compare_df[1:50, ]
```




```{r}
# Converting names to initials
compare_df_viz$a <- gsub("(?<=[A-Z])[^A-Z]+", "", compare_df_viz$a ,perl = TRUE)

compare_df_viz$b <- gsub("(?<=[A-Z])[^A-Z]+", "", compare_df_viz$b ,perl = TRUE)
```



```{r}
fig <- plot_ly(compare_df_viz, x = ~a, y = ~b, z = ~score, color=~score, size=~score)
fig <- fig |> add_markers()
fig <- fig |> layout(scene = list(xaxis = list(title = 'Doc1'),
                     yaxis = list(title = 'Doc2'),
                     zaxis = list(title = 'Similarity Score')
                     ))

fig
```



**Similarity Measure: Ratio of Matches**

TODO: construct comparing dataframe and plot3d but using ration of maches as similarity measure


## **Extra Material: Distance Matrix**

Since the [textreuse](https://docs.ropensci.org/textreuse/) library doesn't output a distance matrix, and instead it provides two columns of documents' names, and their similarity score (computed from Jaccard similarity) in the third document, we implemented the transformation of the dataframe to a distance matrix in the following





First we pivot the score dataframe to construct a distance matrix

```{r} 
distance_df <- compare_df |> pivot_wider(names_from=a, values_from=score)

distance_df <- replace(distance_df, is.na(distance_df), 0)

distance_mat <- data.matrix(distance_df)
```



Moreover, the library doesn't provide a function a compute cosine similarity, between two documents, in below we implemented cosine similarity between two documents, and then construct a distance matrix for all documents of the corpus.


```{r}
# compute cosine similarity between two documents
dtms[,1]
cosine_sim <- tcrossprod_simple_triplet_matrix(dtms[,1], dtms[,2])/sqrt(row_sums(dtms[,2]^2) %*% t(row_sums(dtms[,1]^2)))

```

```{r}
# construct cosine distance matrix
cosine_dist_mat <- 1 - crossprod_simple_triplet_matrix(dtms)/(sqrt(col_sums(dtms^2) %*% t(col_sums(dtms^2))))

cosine_dist_mat
```


# **Conclusion** {#conclusion}

//TODO: writing